diff --git a/.gitignore b/.gitignore
index 5c25314..d374e8b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -18,4 +18,6 @@ target
 #  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
 #  and can be added to the global gitignore or merged into this file.  For a more nuclear
 #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
-#.idea/
\ No newline at end of file
+#.idea/
+
+*_gitignore*
\ No newline at end of file
diff --git a/Cargo.lock b/Cargo.lock
index cbe327b..9f52ac9 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -20,13 +20,6 @@ version = "1.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bbd2bcb4c963f2ddae06a2efc7e9f3591312473c50c6685e1f298068316e66fe"
 
-[[package]]
-name = "lispy"
-version = "0.1.0"
-dependencies = [
- "logos",
-]
-
 [[package]]
 name = "logos"
 version = "0.15.1"
@@ -116,3 +109,10 @@ name = "unicode-ident"
 version = "1.0.19"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f63a545481291138910575129486daeaf8ac54aee4387fe7906919f7830c7d9d"
+
+[[package]]
+name = "yelra"
+version = "0.1.0"
+dependencies = [
+ "logos",
+]
diff --git a/Cargo.toml b/Cargo.toml
index 09af03d..0f6e80d 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -1,5 +1,7 @@
+# Cargo.toml
+
 [package]
-name = "lispy"
+name = "yelra"
 version = "0.1.0"
 edition = "2024"
 
diff --git a/README.md b/README.md
index b7794e2..a7a139d 100644
--- a/README.md
+++ b/README.md
@@ -320,6 +320,8 @@ We love precise experiments and sharp edges. To contribute:
 * [ ] 0.10 Interop (Rust/C/WASM/TS)
 * [ ] 1.0 Spec + stabilization
 
+---
+
 # Comparative Landscape: yelra vs. Today’s Majors
 
 > This section is standalone—drop it into the README as-is. It frames where yelra sits relative to C, C++, C#, Java, JavaScript/TypeScript, and Python, focusing on memory, concurrency, effects, protocols/typestate, units/time, interop, and build determinism.
diff --git a/src/ast.rs b/src/ast.rs
index 8dace55..240b348 100644
--- a/src/ast.rs
+++ b/src/ast.rs
@@ -4,3 +4,47 @@ pub enum Expr {
     Symbol(String),
     List(Vec<Expr>),
 }
+
+pub fn eval(expr: &Expr) -> Result<f64, String> {
+    match expr {
+        Expr::Number(n) => Ok(*n),
+        Expr::List(items) => {
+            if items.is_empty() {
+                return Err("Empty list".to_string());
+            }
+            match &items[0] {
+                Expr::Symbol(op) => {
+                    let args: Result<Vec<f64>, String> = items[1..].iter().map(eval).collect();
+                    let args = args?;
+                    match op.as_str() {
+                        "+" => Ok(args.iter().sum()),
+                        "-" => {
+                            if args.len() == 1 {
+                                Ok(-args[0])
+                            } else {
+                                Ok(args[0] - args[1..].iter().sum::<f64>())
+                            }
+                        }
+                        "*" => Ok(args.iter().product()),
+                        "/" => {
+                            if args.len() == 1 {
+                                Ok(1.0 / args[0])
+                            } else {
+                                args[1..].iter().try_fold(args[0], |acc, x| {
+                                    if *x == 0.0 {
+                                        Err("Division by zero".to_string())
+                                    } else {
+                                        Ok(acc / x)
+                                    }
+                                })
+                            }
+                        }
+                        _ => Err(format!("Unknown operator: {}", op)),
+                    }
+                }
+                _ => Err("List must start with a symbol".to_string()),
+            }
+        }
+        Expr::Symbol(s) => Err(format!("Cannot evaluate bare symbol: {}", s)),
+    }
+}
diff --git a/src/lexer.rs b/src/lexer.rs
index fb67832..43b3256 100644
--- a/src/lexer.rs
+++ b/src/lexer.rs
@@ -1,3 +1,5 @@
+// lexer.rs
+
 use logos::Logos;
 
 #[derive(Logos, Debug, Clone, PartialEq)]
@@ -11,11 +13,11 @@ pub enum Token {
     #[regex(r"-?[0-9]+(\.[0-9]+)?", |lex| lex.slice().to_string(), priority = 3)]
     Number(String),
 
-    // Symbols (operators and identifiers)
-    #[regex(r"[A-Za-z_+\-*/=<>!][A-Za-z0-9_+\-*/=<>!]*", |lex| lex.slice().to_string())]
+    // Operators and Identifiers (merged into one Symbol variant)
+    #[regex(r"[+\-*/=<>!]+|[A-Za-z_][A-Za-z0-9_]*", |lex| lex.slice().to_string(), priority = 2)]
     Symbol(String),
 
-    // A variant matched by logos; we'll drop instances of it inside tokenize()
+    // Skip whitespace
     #[regex(r"[ \t\r\n]+", logos::skip)]
     Whitespace,
 }
@@ -26,8 +28,6 @@ pub fn tokenize(input: &str) -> Result<Vec<Token>, String> {
 
     while let Some(res) = lexer.next() {
         match res {
-            // logos::skip should skip, but some logos versions may still yield the variant.
-            // defensively ignore it here.
             Ok(Token::Whitespace) => continue,
             Ok(tok) => tokens.push(tok),
             Err(_) => {
diff --git a/src/main.rs b/src/main.rs
index 6acded0..8f46a15 100644
--- a/src/main.rs
+++ b/src/main.rs
@@ -1,13 +1,15 @@
+// src/main.rs
+
+mod ast;
 mod lexer;
 mod parser;
-mod ast;
 
-use std::io::{self, Write};
 use lexer::tokenize;
 use parser::parse;
+use std::io::{self, Write};
 
 fn main() {
-    println!("Lispy v0.1 — type 'exit' or Ctrl+D to quit");
+    println!("yelra v0.1 — type 'exit' or Ctrl+D to quit");
 
     let stdin = io::stdin();
     loop {
@@ -25,7 +27,10 @@ fn main() {
 
         match tokenize(input) {
             Ok(tokens) => match parse(tokens) {
-                Ok(expr) => println!("AST: {:?}", expr),
+                Ok(expr) => match ast::eval(&expr) {
+                    Ok(val) => println!("{}", val),
+                    Err(e) => println!("Eval error: {}", e),
+                },
                 Err(e) => println!("Parse error: {}", e),
             },
             Err(e) => println!("Lex error: {}", e),
diff --git a/src/parser.rs b/src/parser.rs
index bad4eda..ee29b15 100644
--- a/src/parser.rs
+++ b/src/parser.rs
@@ -1,3 +1,5 @@
+// parser.rs
+
 use crate::ast::Expr;
 use crate::lexer::Token;
 
@@ -8,6 +10,11 @@ pub struct Parser {
 
 impl Parser {
     pub fn new(tokens: Vec<Token>) -> Self {
+        let tokens: Vec<Token> = tokens
+            .into_iter()
+            .filter(|t| !matches!(t, Token::Whitespace))
+            .collect();
+
         Parser { tokens, pos: 0 }
     }
 
@@ -29,7 +36,9 @@ impl Parser {
         // Parse a primary expression first
         let first = match self.next() {
             Some(Token::Number(s)) => {
-                let n: f64 = s.parse().map_err(|e| format!("Invalid number '{}': {}", s, e))?;
+                let n: f64 = s
+                    .parse()
+                    .map_err(|e| format!("Invalid number '{}': {}", s, e))?;
                 Expr::Number(n)
             }
             Some(Token::Symbol(s)) => {
@@ -52,6 +61,7 @@ impl Parser {
                 return Err("Unclosed '(' — reached end of input".to_string());
             }
             Some(Token::RParen) => return Err("Unexpected ')'".to_string()),
+            Some(Token::Whitespace) => return Err("Unexpected whitespace token".to_string()),
             None => return Err("Unexpected end of input".to_string()),
         };
 
@@ -62,18 +72,21 @@ impl Parser {
                 let mut operands: Vec<Expr> = vec![first.clone()];
                 let mut ops: Vec<String> = Vec::new();
 
-                // try to collect (op, rhs) pairs
+                // collect (op, rhs) pairs greedily, but only when the next token is a Symbol
                 loop {
-                    // if next token is a Symbol, consume it and parse rhs
-                    if let Some(Token::Symbol(op)) = self.peek().cloned() {
-                        // consume operator
-                        self.next();
-                        // parse rhs expression
-                        let rhs = self.parse_expr()?;
-                        ops.push(op);
-                        operands.push(rhs);
-                    } else {
-                        break;
+                    match self.peek() {
+                        Some(Token::Symbol(_)) => {
+                            // consume operator symbol
+                            let op = match self.next() {
+                                Some(Token::Symbol(s)) => s,
+                                _ => unreachable!(),
+                            };
+                            // parse rhs expression
+                            let rhs = self.parse_expr()?;
+                            ops.push(op);
+                            operands.push(rhs);
+                        }
+                        _ => break,
                     }
                 }
 
@@ -88,7 +101,7 @@ impl Parser {
                     list.extend(operands.into_iter());
                     Ok(Expr::List(list))
                 } else {
-                    // multiple operators: ensure they are all the same
+                    // multiple operators: ensure they are all the same (left-assoc, same-op only)
                     let all_same = ops.iter().all(|o| o == &ops[0]);
                     if all_same {
                         let op0 = ops[0].clone();
@@ -112,8 +125,22 @@ impl Parser {
 pub fn parse(tokens: Vec<Token>) -> Result<Expr, String> {
     let mut p = Parser::new(tokens);
     let expr = p.parse_expr()?;
-    if p.peek().is_some() {
-        return Err("Extra tokens after first expression".to_string());
+
+    // Ensure we've consumed all tokens; if anything remains, report where we stopped.
+    if let Some(remaining) = p.peek() {
+        // Give a clearer message for debugging leftover tokens
+        let kind = match remaining {
+            Token::LParen => "('(')".to_string(),
+            Token::RParen => "')'".to_string(),
+            Token::Number(n) => format!("number `{}`", n),
+            Token::Symbol(s) => format!("symbol `{}`", s),
+            Token::Whitespace => "whitespace".to_string(),
+        };
+        return Err(format!(
+            "Extra tokens after first expression (next token: {})",
+            kind
+        ));
     }
+
     Ok(expr)
 }
diff --git a/src/prompt.txt b/src/prompt.txt
deleted file mode 100644
index 9921461..0000000
--- a/src/prompt.txt
+++ /dev/null
@@ -1,142 +0,0 @@
-FIX ERROR:
-
-   Compiling lispy v0.1.0 (C:\Users\jramirez\OneDrive - Concepcion Industrial Corporation\Documents\workspace\rust\lispy)
-    Finished `dev` profile [unoptimized + debuginfo] target(s) in 16.48s
-     Running `target\debug\lispy.exe`
-Lispy v0.1 — type 'exit' or Ctrl+D to quit
-> 1 + 1
-Parse error: Extra tokens after first expression
-> 1+1
-Parse error: Extra tokens after first expression
-> 1 + 1
-Parse error: Extra tokens after first expression
->
-
-
-
-# lexers.rs
-
-use logos::Logos;
-
-#[derive(Logos, Debug, Clone, PartialEq)]
-pub enum Token {
-    #[token("(")]
-    LParen,
-    #[token(")")]
-    RParen,
-
-    // Numbers (priority beats Symbol)
-    #[regex(r"-?[0-9]+(\.[0-9]+)?", |lex| lex.slice().to_string(), priority = 3)]
-    Number(String),
-
-    // Symbols (operators and identifiers)
-    #[regex(r"[A-Za-z_+\-*/=<>!][A-Za-z0-9_+\-*/=<>!]*", |lex| lex.slice().to_string())]
-    Symbol(String),
-
-    // A variant matched by logos; we'll drop instances of it inside tokenize()
-    #[regex(r"[ \t\r\n]+", logos::skip)]
-    Whitespace,
-}
-
-pub fn tokenize(input: &str) -> Result<Vec<Token>, String> {
-    let mut lexer = Token::lexer(input);
-    let mut tokens = Vec::new();
-
-    while let Some(res) = lexer.next() {
-        match res {
-            // logos::skip should skip, but some logos versions may still yield the variant.
-            // defensively ignore it here.
-            Ok(Token::Whitespace) => continue,
-            Ok(tok) => tokens.push(tok),
-            Err(_) => {
-                let span = lexer.span();
-                return Err(format!("Unexpected token at {}..{}", span.start, span.end));
-            }
-        }
-    }
-
-    Ok(tokens)
-}
-
-
-
-
-
-
-
-
-
-
-
-# parser.rs
-
-use crate::ast::Expr;
-use crate::lexer::Token;
-
-pub struct Parser {
-    tokens: Vec<Token>,
-    pos: usize,
-}
-
-impl Parser {
-    pub fn new(tokens: Vec<Token>) -> Self {
-        Parser { tokens, pos: 0 }
-    }
-
-    fn peek(&self) -> Option<&Token> {
-        self.tokens.get(self.pos)
-    }
-
-    fn next(&mut self) -> Option<Token> {
-        if self.pos < self.tokens.len() {
-            let t = self.tokens[self.pos].clone();
-            self.pos += 1;
-            Some(t)
-        } else {
-            None
-        }
-    }
-
-    pub fn parse_expr(&mut self) -> Result<Expr, String> {
-        match self.next() {
-            // If we see whitespace (if your lexer still emits it), skip it and keep parsing.
-            Some(Token::Whitespace) => return self.parse_expr(),
-
-            Some(Token::Number(s)) => {
-                let n: f64 = s.parse().map_err(|e| format!("Invalid number '{}': {}", s, e))?;
-                Ok(Expr::Number(n))
-            }
-
-            Some(Token::Symbol(s)) => Ok(Expr::Symbol(s)),
-
-            Some(Token::LParen) => {
-                let mut exprs = Vec::new();
-                while let Some(tok) = self.peek() {
-                    if matches!(tok, Token::RParen) {
-                        // consume RParen
-                        self.next();
-                        return Ok(Expr::List(exprs));
-                    } else {
-                        let e = self.parse_expr()?;
-                        exprs.push(e);
-                    }
-                }
-                Err("Unclosed '(' — reached end of input".to_string())
-            }
-
-            Some(Token::RParen) => Err("Unexpected ')'".to_string()),
-
-            None => Err("Unexpected end of input".to_string()),
-        }
-    }
-
-}
-
-pub fn parse(tokens: Vec<Token>) -> Result<Expr, String> {
-    let mut p = Parser::new(tokens);
-    let expr = p.parse_expr()?;
-    if p.peek().is_some() {
-        return Err("Extra tokens after first expression".to_string());
-    }
-    Ok(expr)
-}
