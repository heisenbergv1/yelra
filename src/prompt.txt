FIX ERROR:

   Compiling lispy v0.1.0 (C:\Users\jramirez\OneDrive - Concepcion Industrial Corporation\Documents\workspace\rust\lispy)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 16.48s
     Running `target\debug\lispy.exe`
Lispy v0.1 — type 'exit' or Ctrl+D to quit
> 1 + 1
Parse error: Extra tokens after first expression
> 1+1
Parse error: Extra tokens after first expression
> 1 + 1
Parse error: Extra tokens after first expression
>



# lexers.rs

use logos::Logos;

#[derive(Logos, Debug, Clone, PartialEq)]
pub enum Token {
    #[token("(")]
    LParen,
    #[token(")")]
    RParen,

    // Numbers (priority beats Symbol)
    #[regex(r"-?[0-9]+(\.[0-9]+)?", |lex| lex.slice().to_string(), priority = 3)]
    Number(String),

    // Symbols (operators and identifiers)
    #[regex(r"[A-Za-z_+\-*/=<>!][A-Za-z0-9_+\-*/=<>!]*", |lex| lex.slice().to_string())]
    Symbol(String),

    // A variant matched by logos; we'll drop instances of it inside tokenize()
    #[regex(r"[ \t\r\n]+", logos::skip)]
    Whitespace,
}

pub fn tokenize(input: &str) -> Result<Vec<Token>, String> {
    let mut lexer = Token::lexer(input);
    let mut tokens = Vec::new();

    while let Some(res) = lexer.next() {
        match res {
            // logos::skip should skip, but some logos versions may still yield the variant.
            // defensively ignore it here.
            Ok(Token::Whitespace) => continue,
            Ok(tok) => tokens.push(tok),
            Err(_) => {
                let span = lexer.span();
                return Err(format!("Unexpected token at {}..{}", span.start, span.end));
            }
        }
    }

    Ok(tokens)
}











# parser.rs

use crate::ast::Expr;
use crate::lexer::Token;

pub struct Parser {
    tokens: Vec<Token>,
    pos: usize,
}

impl Parser {
    pub fn new(tokens: Vec<Token>) -> Self {
        Parser { tokens, pos: 0 }
    }

    fn peek(&self) -> Option<&Token> {
        self.tokens.get(self.pos)
    }

    fn next(&mut self) -> Option<Token> {
        if self.pos < self.tokens.len() {
            let t = self.tokens[self.pos].clone();
            self.pos += 1;
            Some(t)
        } else {
            None
        }
    }

    pub fn parse_expr(&mut self) -> Result<Expr, String> {
        match self.next() {
            // If we see whitespace (if your lexer still emits it), skip it and keep parsing.
            Some(Token::Whitespace) => return self.parse_expr(),

            Some(Token::Number(s)) => {
                let n: f64 = s.parse().map_err(|e| format!("Invalid number '{}': {}", s, e))?;
                Ok(Expr::Number(n))
            }

            Some(Token::Symbol(s)) => Ok(Expr::Symbol(s)),

            Some(Token::LParen) => {
                let mut exprs = Vec::new();
                while let Some(tok) = self.peek() {
                    if matches!(tok, Token::RParen) {
                        // consume RParen
                        self.next();
                        return Ok(Expr::List(exprs));
                    } else {
                        let e = self.parse_expr()?;
                        exprs.push(e);
                    }
                }
                Err("Unclosed '(' — reached end of input".to_string())
            }

            Some(Token::RParen) => Err("Unexpected ')'".to_string()),

            None => Err("Unexpected end of input".to_string()),
        }
    }

}

pub fn parse(tokens: Vec<Token>) -> Result<Expr, String> {
    let mut p = Parser::new(tokens);
    let expr = p.parse_expr()?;
    if p.peek().is_some() {
        return Err("Extra tokens after first expression".to_string());
    }
    Ok(expr)
}
